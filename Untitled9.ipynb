{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9b6754-4e64-4591-b2db-0a44b9632725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"pranaysaggar/flan_t-5_story_summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a0f061-4a29-4631-9492-39aa35a2053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Once there was a team who was told to present in front of the crowd. their laptops stoped working making them a laughing stock. but this didn't break their hopes as they continued to present. suddenly a Tiger entered the room and ate everyone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f0dcfe-047b-442f-b630-6359f30b6f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story summarisation:\n",
      "[{'summary_text': 'A team is told to present in front of a crowd, but their laptops stop working and a Tiger enters the room and eats everyone.'}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summary_story = summarizer(text)\n",
    "print(\"Generated Story summarisation:\")\n",
    "print(summary_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1496a8a8-dbbc-40ab-8b9b-da87bfaed3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A team is told to present in front of a crowd, but their laptops stop working and a Tiger enters the room and eats everyone.\n"
     ]
    }
   ],
   "source": [
    "sum_text = summary_story[0]['summary_text']\n",
    "print(sum_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30c11b5-ca2f-41c1-b6e6-40a1ab46a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "genre_tokenizer = AutoTokenizer.from_pretrained(\"pranaysaggar/GIST_small_genre_categorizer\")\n",
    "genre_model = AutoModelForSequenceClassification.from_pretrained(\"pranaysaggar/GIST_small_genre_categorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8db22fa-6482-4604-a9f4-0caec561ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mapping = {\n",
    "    0: 'Comedy & Satire',\n",
    "    1: 'Drama & Romance',\n",
    "    2: 'Historical & Period',\n",
    "    3: 'Mystery & Thriller',\n",
    "    4: 'Science Fiction & Fantasy'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d587fe-2037-404b-8745-df4f23c64549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_genre(story):\n",
    "    # Tokenize the story\n",
    "    inputs = genre_tokenizer(story, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = genre_model(**inputs)\n",
    "    \n",
    "    # Get predicted genre\n",
    "    predicted_genre_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    # Map the predicted genre ID to the actual genre label\n",
    "    predicted_genre = genre_mapping[predicted_genre_id]\n",
    "\n",
    "    return predicted_genre\n",
    "story_genre=classify_genre(sum_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "344d4a48-1d5b-4ead-abcd-80ae93b451ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comedy & Satire\n"
     ]
    }
   ],
   "source": [
    "print(story_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7374b35f-a0ae-4ebd-88e2-d1c0d6f75722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b16313f71143508582f9dd4ad6a249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c2755a-099c-41b2-9228-e2d016a9c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 14:17:49.529921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-09 14:17:55.446665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2c40a9941b4e90b2e02625c3ae0dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer,  AutoModelForCausalLM\n",
    "# gen_model_name=\"pranaysaggar/story_generator_llama2\"\n",
    "# gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "# gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name)\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\",device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9fdf6a-e7bc-4d89-bfb2-b11efb8b4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_genre2=\"Drama & Romance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67fefd37-f9c1-4107-993d-7190de67e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"complete the story in psychotic triller way: \"+sum_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0beb27cd-8e81-4865-b7ae-05f19348f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"<BOS> <\"+story_genre2+\"> \"+sum_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01e2fd48-3cd8-48fa-b3b4-43481b3e72da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated story: complete the story in psychotic way: A team is told to present in front of a crowd, but their laptops stop working and a Tiger enters the room and eats everyone...., clean organ sp y y y y need need need need sp need. have have need returning to make.... need color color.... need need need... need need need.... need need need need... need need need need.\n"
     ]
    }
   ],
   "source": [
    "# inputs = gen_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# outputs = gen_model.generate(inputs, max_length=100, num_return_sequences=1)\n",
    "# decoded_output = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(\"Generated story:\", decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cc61faf-f402-46a5-a70f-83c6f6685df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"complete the story in psychotic triller way: A team is told to present in front of a crowd, but their laptops stop working and a Tiger enters the room and eats everyone.\\n\\nYou could also add some twist to the story to make it more interesting:\\n\\n* The team is not just any team, but a group of scientists who are working on a top-secret project.\\n* The presentation is not just any presentation, but a pitch for a new technology that could change the world.\\n* The tiger is not just any tiger, but a genetically modified creature created by the scientists themselves.\\n* The team members are not just any people, but clones created by the scientists to work on the project.\\n* The presentation is not just any presentation, but a test of the team's abilities to work under pressure.\\n* The tiger is not just any tiger, but a symbol of the unpredictability of nature and the dangers of playing with forces beyond our control.\\n\\nYou could also add some red herrings to make the story more intriguing:\\n\\n* The team members start to act strange and erratic before the presentation, leading the reader to suspect that something is wrong with them.\\n* A mysterious figure is seen lurking around the room before the tiger enters, leading the reader to suspect that someone is trying to sabotage the presentation.\\n* The tiger is not just any tiger, but a creature that has been genetically modified to be intelligent and sentient, leading the reader to suspect that there is more to the creature than meets the eye.\\n\\nYou could also add some foreshadowing to make the story more ominous:\\n\\n* The team members start to notice strange occurrences happening around them before the presentation, such as equipment malfunctioning or strange noises in the hallway.\\n* The scientists are seen arguing and tension is building up in the days leading to the presentation.\\n* The team leader is seen receiving mysterious messages before the presentation, hinting that something is going to go wrong.\\n\\nYou could also add some symbolism to the story to make it more profound:\\n\\n* The tiger could represent the unpredictability of nature and the dangers of playing with forces beyond our control.\\n* The presentation could represent the pressure to innovate and the dangers of relying too heavily on technology.\\n* The team members could represent the different aspects of the human psyche and the struggle to work together under pressure.\\n\\nYou could also add some twist at the end to make the story more unexpected:\\n\\n* The tiger is revealed to be a hallucination caused by a side effect of the new technology being presented.\\n* The team members are revealed to be clones created by a rival company to sabotage the presentation.\\n* The presentation is revealed to be a test of the team's abilities to work under pressure, and they pass with flying colors.\\n\\nYou could also add some moral to the story to make it more thought-provoking:\\n\\n* The story could be a warning about the dangers of playing with forces beyond our control and the importance of respecting nature.\\n* The story could be a commentary on the pressure to innovate and the dangers of relying too heavily on technology.\\n* The story could be a reflection on the nature of the human psyche and the importance of working together under pressure.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(pipe(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5fc1a-1893-4b41-91cb-701c84b7bd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
